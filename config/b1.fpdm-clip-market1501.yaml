project_name: 'market1501-fusion-CLIP-b48-r10-p005'
root_path: './dataset/market1501/'
device: [0]
phase: 'train'
disable_logger: false
mode: 'client'
port: 52162
batch_size: 48 #70 # 20
num_workers: 16
combiner_hidden_dim: 768  # large 768 base 512
img_pose_drop_rate: 0.0
thr_ratio: 1.0
lr: 0.0
accumulate_grad_batches: 1
scheduler_t0: 10
scheduler_t_mult: 2
scheduler_eta_max: 0.00001
scheduler_t_up: 2
scheduler_gamma: 0.5
temperature: 0.07
weight_decay: 0.0001
max_epochs: 10
img_size: [176, 256] #[256, 256]
lambda_l1: 0.0001
encoder_type: 'clip' # 'clip'
attn_hidden_dim: 1024  # large-1024 base 768
mh_attn_size: 32
img_encoder_update: true
trained_model_name: null
wandb_id: null
train_dataset_name: 'train_pairs_data.json'
test_dataset_name: 'test_pairs_data.json'
img_encoder_path: 'openai/clip-vit-large-patch14' # 'facebook/dinov2-base' # 'openai/clip-vit-base-patch16'
conbiner_self_learning: true
patch_self_learning: false #
learning_encoder_type: 'all' # 'each',
train_patch_embeddings: true
train_patch_embeddings_sampling_ratio: 0.05 # 0.12 # 0.12 # 0.05
